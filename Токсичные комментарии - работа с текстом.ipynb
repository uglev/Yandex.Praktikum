{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Мешок-слов\" data-toc-modified-id=\"Мешок-слов-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Мешок слов</a></span></li><li><span><a href=\"#Построение-моделей-с-применением-BERT\" data-toc-modified-id=\"Построение-моделей-с-применением-BERT-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Построение моделей с применением BERT</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: pytorch_pretrained_bert in c:\\users\\uglev\\anaconda3\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: requests in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (2.25.1)\n",
      "Requirement already satisfied: regex in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (2021.4.4)\n",
      "Requirement already satisfied: boto3 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (1.18.62)\n",
      "Requirement already satisfied: numpy in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (1.20.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (4.59.0)\n",
      "Requirement already satisfied: torch>=0.4.1 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pytorch_pretrained_bert) (1.9.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.62 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from boto3->pytorch_pretrained_bert) (1.21.62)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from boto3->pytorch_pretrained_bert) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from botocore<1.22.0,>=1.21.62->boto3->pytorch_pretrained_bert) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from botocore<1.22.0,>=1.21.62->boto3->pytorch_pretrained_bert) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.62->boto3->pytorch_pretrained_bert) (1.15.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->pytorch_pretrained_bert) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->pytorch_pretrained_bert) (2020.12.5)\n",
      "Requirement already satisfied: catboost in c:\\users\\uglev\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from catboost) (1.6.2)\n",
      "Requirement already satisfied: graphviz in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from catboost) (0.17)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from catboost) (1.2.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from catboost) (3.3.4)\n",
      "Requirement already satisfied: six in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from catboost) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from catboost) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2021.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (8.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\uglev\\anaconda3\\lib\\site-packages (3.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from lightgbm) (1.20.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from lightgbm) (0.24.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from lightgbm) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\uglev\\anaconda3\\lib\\site-packages (4.11.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (0.0.19)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: filelock in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: pymystem3 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from pymystem3) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->pymystem3) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->pymystem3) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\uglev\\anaconda3\\lib\\site-packages (from requests->pymystem3) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2\n",
    "!pip install pytorch_pretrained_bert\n",
    "!pip install catboost --no-cache-dir\n",
    "!pip install lightgbm\n",
    "!pip install transformers\n",
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем всё как обычно. Доустановим неустановленное, импортируем библиотеки, загрузим датасет..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Uglev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Uglev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Uglev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymorphy2\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('toxic_comments.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143346\n",
       "1     16225\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8.834884437596301"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df['toxic'].value_counts())\n",
    "class_ratio = df['toxic'].value_counts()[0] / df['toxic'].value_counts()[1]\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдается дисбаланс классов. Это наблюдение даст нам значение accuracy для оценки адекватности моделей: она должна быть больше 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В столбце text приведем все сообщения к нижнему регистру. Создадим копию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text = df.text.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизируем текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_tf_idf = RegexpTokenizer(r'\\w{2,}')\n",
    "def clean_stemm(text):\n",
    "    \"\"\"Функция, отвечающая за лемматизацию слов корпуса\n",
    "    \"\"\"\n",
    "    #new_words = nltk.word_tokenize(text)\n",
    "    #new_words = tokenizer_tf_idf.tokenize(text)\n",
    "    # Инициализация лемматизатора\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Лемматизация корпуса\n",
    "    return \" \".join([lemmatizer.lemmatize(w,\"n\") for w in nltk.word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_tf_idf['stem_text'] = df_tf_idf['text'].apply(clean_stemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим датафрейм на признак и на целевой признак, далее разделим их на обучающую и тестовую выборки. Выведем информацию о размерах выборок, чтобы убедиться в правильной разбивке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк в обучающей выборке: 127656\n",
      "Количество строк в тестовой выборке: 31915\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus_tf_idf = df_tf_idf['stem_text'].values.astype('U')\n",
    "target_tf_idf = df_tf_idf['toxic']\n",
    "\n",
    "train_features_tf_idf, test_features_tf_idf, train_target_tf_idf, test_target_tf_idf = train_test_split(\n",
    "corpus_tf_idf, target_tf_idf, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Количество строк в обучающей выборке:', train_features_tf_idf.shape[0])\n",
    "print('Количество строк в тестовой выборке:', test_features_tf_idf.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим стоп-слова и посмотрим на размер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "tf_idf_train = count_tf_idf.fit_transform(train_features_tf_idf)\n",
    "#tf_idf_test = count_tf_idf.transform(test_features_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И применим логистическую регрессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uglev\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [31915, 127656]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    263\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [31915, 127656]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_lr_tf_idf = LogisticRegression(random_state=12345, solver='lbfgs', class_weight = 'balanced')\n",
    "model_lr_tf_idf.fit(tf_idf_train, train_target_tf_idf)\n",
    "predicted_lr_tf_idf = model_lr_tf_idf.predict(tf_idf_test)\n",
    "accuracy_lr_tf_idf = accuracy_score(test_target_tf_idf, predicted_lr_tf_idf)\n",
    "f1_score_lr_tf_idf = f1_score(test_target_tf_idf, predicted_lr_tf_idf)\n",
    "\n",
    "probabilities_test_lr_tf_idf = model_lr_tf_idf.predict_proba(tf_idf_test)\n",
    "probabilities_one_test_lr_tf_idf = probabilities_test_lr_tf_idf[:, 1]\n",
    "auc_roc_lr_tf_idf = roc_auc_score(test_target_tf_idf, probabilities_one_test_lr_tf_idf)\n",
    "\n",
    "print('Логистическая регрессия:')\n",
    "print('accuracy:', accuracy_lr_tf_idf.round(decimals=3))\n",
    "print('f1_score:', f1_score_lr_tf_idf.round(decimals=3))\n",
    "print('auc_roc:', auc_roc_lr_tf_idf.round(decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель catBoost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4910376\ttotal: 820ms\tremaining: 13m 39s\n",
      "200:\tlearn: 0.8113133\ttotal: 1m 57s\tremaining: 7m 47s\n",
      "400:\tlearn: 0.8371017\ttotal: 3m 53s\tremaining: 5m 48s\n",
      "600:\tlearn: 0.8489900\ttotal: 5m 49s\tremaining: 3m 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_cat_tf_idf = CatBoostClassifier(iterations=1000,\n",
    "                                      random_seed = 42,\n",
    "                                      metric_period = 50,\n",
    "                                      eval_metric = 'F1',\n",
    "                                      learning_rate = 0.5,\n",
    "                                      early_stopping_rounds = 50, \n",
    "                                      #task_type=\"GPU\", \n",
    "                                      #devices='0:1'\n",
    "                                     )\n",
    "model_cat_tf_idf.fit(tf_idf_train, train_target_tf_idf, verbose=200)\n",
    "predicted_cat_tf_idf = model_cat_tf_idf.predict(tf_idf_test)\n",
    "accuracy_cat_tf_idf = accuracy_score(test_target_tf_idf, predicted_cat_tf_idf)\n",
    "f1_score_cat_tf_idf = f1_score(test_target_tf_idf, predicted_cat_tf_idf)\n",
    "\n",
    "probabilities_test_cat_tf_idf = model_cat_tf_idf.predict_proba(tf_idf_test)\n",
    "probabilities_one_test_cat_tf_idf = probabilities_test_cat_tf_idf[:, 1]\n",
    "auc_roc_cat_tf_idf = roc_auc_score(test_target_tf_idf, probabilities_one_test_cat_tf_idf)\n",
    "\n",
    "print('CatBoost:')\n",
    "print('accuracy:', accuracy_cat_tf_idf.round(decimals=3))\n",
    "print('f1_score:', f1_score_cat_tf_idf.round(decimals=3))\n",
    "print('auc_roc:', auc_roc_cat_tf_idf.round(decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим LightGBM на стандартных настройках гиперпараметров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_lgb_tf_idf = lgb.LGBMClassifier()\n",
    "model_lgb_tf_idf.fit(tf_idf_train, train_target_tf_idf)\n",
    "predicted_lgb_tf_idf = model_lgb_tf_idf.predict(tf_idf_test)\n",
    "accuracy_lgb_tf_idf = accuracy_score(test_target_tf_idf, predicted_lgb_tf_idf)\n",
    "f1_score_lgb_tf_idf = f1_score(test_target_tf_idf, predicted_lgb_tf_idf)\n",
    "\n",
    "probabilities_test_lgb_tf_idf = model_lgb_tf_idf.predict_proba(tf_idf_test)\n",
    "probabilities_one_test_lgb_tf_idf = probabilities_test_lgb_tf_idf[:, 1]\n",
    "auc_roc_lgb_tf_idf = roc_auc_score(test_target_tf_idf, probabilities_one_test_lgb_tf_idf)\n",
    "\n",
    "print('LightGBM:')\n",
    "print('accuracy:', accuracy_lgb_tf_idf.round(decimals=3))\n",
    "print('f1_score:', f1_score_lgb_tf_idf.round(decimals=3))\n",
    "print('auc_roc:', auc_roc_lgb_tf_idf.round(decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Построение моделей с применением BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert_ones = df[df['toxic']==1].sample(1000).reset_index(drop=True)\n",
    "df_bert_zeros = df[df['toxic']==0].sample(1000).reset_index(drop=True)\n",
    "df_bert = pd.concat([df_bert_ones] + [df_bert_zeros]).reset_index(drop=True)\n",
    "df_bert = shuffle(df_bert, random_state=12345).reset_index(drop=True)\n",
    "\n",
    "df_bert.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим токенайзер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ppb.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем текст каждого твита."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df_bert['text'].apply((lambda x: tokenizer.encode(x, max_length=512, add_special_tokens=True)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализируем модель из файла:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppb.BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = ppb.BertModel.from_pretrained('bert-base-uncased', config = config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполними нулями каждый массив токенов до длины максимального массива, чтобы получить матрицу одной длины. Наложим маску на значимые токены. Нам важны все слова кроме нулевых токенов, появившихся на предыдущем шаге паддинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list = tokenized.apply(lambda x: len(x))\n",
    "max_len = max(len_list)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем векторы текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        input_ids = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids, attention_mask=attention_mask_batch)\n",
    "\n",
    "        embeddings.append(last_hidden_states[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем список батчей эмбеддингов в numpy-матрицу, в другую матрицу запишем значения целевого признака, и разделим матрицы на обучающую и тестовую выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(embeddings)\n",
    "labels = df_bert['toxic']\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, \n",
    "                                                                            test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_lr = LogisticRegression(solver = 'lbfgs', class_weight = 'balanced', max_iter = 1000)\n",
    "model_lr.fit(train_features, train_labels)\n",
    "predicted_lr = model_lr.predict(test_features)\n",
    "accuracy_lr = accuracy_score(test_labels, predicted_lr)\n",
    "f1_score_lr = f1_score(test_labels, predicted_lr)\n",
    "\n",
    "probabilities_test_lr = model_lr.predict_proba(test_features)\n",
    "probabilities_one_test_lr = probabilities_test_lr[:, 1]\n",
    "auc_roc_lr = roc_auc_score(test_labels, probabilities_one_test_lr)\n",
    "\n",
    "print('Логистическая регрессия:')\n",
    "print('accuracy:', accuracy_lr.round(decimals=3))\n",
    "print('f1_score:', f1_score_lr.round(decimals=3))\n",
    "print('auc_roc:', auc_roc_lr.round(decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_lgb = lgb.LGBMClassifier()\n",
    "model_lgb.fit(train_features, train_labels)\n",
    "predicted_lgb = model_lgb.predict(test_features)\n",
    "accuracy_lgb = accuracy_score(test_labels, predicted_lgb)\n",
    "f1_score_lgb = f1_score(test_labels, predicted_lgb)\n",
    "\n",
    "probabilities_test_lgb = model_lgb.predict_proba(test_features)\n",
    "probabilities_one_test_lgb = probabilities_test_lgb[:, 1]\n",
    "auc_roc_lgb = roc_auc_score(test_labels, probabilities_one_test_lgb)\n",
    "\n",
    "print('LightGBM:')\n",
    "print('accuracy:', accuracy_lgb.round(decimals=3))\n",
    "print('f1_score:', f1_score_lgb.round(decimals=3))\n",
    "print('auc_roc:', auc_roc_lgb.round(decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_cat = CatBoostClassifier(iterations=1000,\n",
    "                               random_seed = 42,\n",
    "                               metric_period = 50,\n",
    "                               eval_metric = 'F1',\n",
    "                               learning_rate = 0.5,\n",
    "                               early_stopping_rounds = 50, \n",
    "                               #task_type=\"GPU\", \n",
    "                               #devices='0:1'\n",
    "                              )\n",
    "\n",
    "model_cat.fit(train_features, train_labels, verbose=200)\n",
    "predicted_cat = model_cat.predict(test_features)\n",
    "accuracy_cat = accuracy_score(test_labels, predicted_cat)\n",
    "f1_score_cat = f1_score(test_labels, predicted_cat)\n",
    "\n",
    "probabilities_test_cat = model_cat.predict_proba(test_features)\n",
    "probabilities_one_test_cat = probabilities_test_cat[:, 1]\n",
    "auc_roc_cat = roc_auc_score(test_labels, probabilities_one_test_cat)\n",
    "\n",
    "print('CatBoost:')\n",
    "print('accuracy:', accuracy_cat.round(decimals=3))\n",
    "print('f1_score:', f1_score_cat.round(decimals=3))\n",
    "print('auc_roc:', auc_roc_cat.round(decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все использованные модели сильно превысили необходимое значение метрики F1=0.75. С применением TF-IDF лучшей моделью по значению метрики F1 является CatBoost (F1=0.78). С применением BERT лучшей моделью по значению метрики F1 является логистическая регрессия (F1=0.865). У нее также самые большие значения Accuracy и ROC-AUC и наименьшее время обучения (< 1 сек) по сравнению с CatBoost и LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3416,
    "start_time": "2021-10-13T16:24:03.884Z"
   },
   {
    "duration": 1731,
    "start_time": "2021-10-13T16:24:07.303Z"
   },
   {
    "duration": 2175,
    "start_time": "2021-10-13T16:24:09.037Z"
   },
   {
    "duration": 19,
    "start_time": "2021-10-13T16:24:45.850Z"
   },
   {
    "duration": 20,
    "start_time": "2021-10-13T16:24:47.302Z"
   },
   {
    "duration": 2127,
    "start_time": "2021-10-13T16:35:40.675Z"
   },
   {
    "duration": 3020,
    "start_time": "2021-10-13T16:39:09.479Z"
   },
   {
    "duration": 1713,
    "start_time": "2021-10-13T16:39:12.502Z"
   },
   {
    "duration": 2422,
    "start_time": "2021-10-13T16:39:14.217Z"
   },
   {
    "duration": 18,
    "start_time": "2021-10-13T16:39:16.642Z"
   },
   {
    "duration": 25,
    "start_time": "2021-10-13T16:39:16.662Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-13T16:39:16.689Z"
   },
   {
    "duration": 68,
    "start_time": "2021-10-13T16:39:16.722Z"
   },
   {
    "duration": 410595,
    "start_time": "2021-10-13T16:39:16.794Z"
   },
   {
    "duration": 50,
    "start_time": "2021-10-13T16:46:07.391Z"
   },
   {
    "duration": 10315,
    "start_time": "2021-10-13T16:46:07.443Z"
   },
   {
    "duration": 706598,
    "start_time": "2021-10-13T16:46:17.760Z"
   },
   {
    "duration": 39339,
    "start_time": "2021-10-13T16:58:04.362Z"
   },
   {
    "duration": 17867,
    "start_time": "2021-10-13T16:58:43.703Z"
   },
   {
    "duration": 3603,
    "start_time": "2021-10-14T07:07:36.659Z"
   },
   {
    "duration": 6862,
    "start_time": "2021-10-14T07:07:40.265Z"
   },
   {
    "duration": 642,
    "start_time": "2021-10-14T07:07:46.489Z"
   },
   {
    "duration": 638,
    "start_time": "2021-10-14T07:07:46.494Z"
   },
   {
    "duration": 638,
    "start_time": "2021-10-14T07:07:46.496Z"
   },
   {
    "duration": 638,
    "start_time": "2021-10-14T07:07:46.498Z"
   },
   {
    "duration": 637,
    "start_time": "2021-10-14T07:07:46.500Z"
   },
   {
    "duration": 637,
    "start_time": "2021-10-14T07:07:46.502Z"
   },
   {
    "duration": 636,
    "start_time": "2021-10-14T07:07:46.504Z"
   },
   {
    "duration": 636,
    "start_time": "2021-10-14T07:07:46.506Z"
   },
   {
    "duration": 635,
    "start_time": "2021-10-14T07:07:46.508Z"
   },
   {
    "duration": 635,
    "start_time": "2021-10-14T07:07:46.510Z"
   },
   {
    "duration": 635,
    "start_time": "2021-10-14T07:07:46.511Z"
   },
   {
    "duration": 634,
    "start_time": "2021-10-14T07:07:46.513Z"
   },
   {
    "duration": 635,
    "start_time": "2021-10-14T07:07:46.514Z"
   },
   {
    "duration": 635,
    "start_time": "2021-10-14T07:07:46.515Z"
   },
   {
    "duration": 634,
    "start_time": "2021-10-14T07:07:46.518Z"
   },
   {
    "duration": 633,
    "start_time": "2021-10-14T07:07:46.520Z"
   },
   {
    "duration": 633,
    "start_time": "2021-10-14T07:07:46.522Z"
   },
   {
    "duration": 633,
    "start_time": "2021-10-14T07:07:46.523Z"
   },
   {
    "duration": 633,
    "start_time": "2021-10-14T07:07:46.525Z"
   },
   {
    "duration": 632,
    "start_time": "2021-10-14T07:07:46.527Z"
   },
   {
    "duration": 633,
    "start_time": "2021-10-14T07:07:46.528Z"
   },
   {
    "duration": 632,
    "start_time": "2021-10-14T07:07:46.530Z"
   },
   {
    "duration": 632,
    "start_time": "2021-10-14T07:07:46.532Z"
   },
   {
    "duration": 632,
    "start_time": "2021-10-14T07:07:46.533Z"
   },
   {
    "duration": 3080,
    "start_time": "2021-10-14T07:09:34.486Z"
   },
   {
    "duration": 6192,
    "start_time": "2021-10-14T07:09:37.570Z"
   },
   {
    "duration": 2143,
    "start_time": "2021-10-14T07:09:43.765Z"
   },
   {
    "duration": 22,
    "start_time": "2021-10-14T07:09:45.912Z"
   },
   {
    "duration": 1094,
    "start_time": "2021-10-14T07:09:45.936Z"
   },
   {
    "duration": 614,
    "start_time": "2021-10-14T07:09:46.420Z"
   },
   {
    "duration": 611,
    "start_time": "2021-10-14T07:09:46.425Z"
   },
   {
    "duration": 611,
    "start_time": "2021-10-14T07:09:46.428Z"
   },
   {
    "duration": 610,
    "start_time": "2021-10-14T07:09:46.431Z"
   },
   {
    "duration": 610,
    "start_time": "2021-10-14T07:09:46.433Z"
   },
   {
    "duration": 611,
    "start_time": "2021-10-14T07:09:46.435Z"
   },
   {
    "duration": 612,
    "start_time": "2021-10-14T07:09:46.436Z"
   },
   {
    "duration": 612,
    "start_time": "2021-10-14T07:09:46.438Z"
   },
   {
    "duration": 612,
    "start_time": "2021-10-14T07:09:46.440Z"
   },
   {
    "duration": 612,
    "start_time": "2021-10-14T07:09:46.442Z"
   },
   {
    "duration": 612,
    "start_time": "2021-10-14T07:09:46.444Z"
   },
   {
    "duration": 609,
    "start_time": "2021-10-14T07:09:46.450Z"
   },
   {
    "duration": 610,
    "start_time": "2021-10-14T07:09:46.451Z"
   },
   {
    "duration": 610,
    "start_time": "2021-10-14T07:09:46.453Z"
   },
   {
    "duration": 611,
    "start_time": "2021-10-14T07:09:46.454Z"
   },
   {
    "duration": 612,
    "start_time": "2021-10-14T07:09:46.455Z"
   },
   {
    "duration": 612,
    "start_time": "2021-10-14T07:09:46.457Z"
   },
   {
    "duration": 612,
    "start_time": "2021-10-14T07:09:46.459Z"
   },
   {
    "duration": 613,
    "start_time": "2021-10-14T07:09:46.460Z"
   },
   {
    "duration": 614,
    "start_time": "2021-10-14T07:09:46.461Z"
   },
   {
    "duration": 615,
    "start_time": "2021-10-14T07:09:46.463Z"
   },
   {
    "duration": 3252,
    "start_time": "2021-10-14T07:10:02.714Z"
   },
   {
    "duration": 5934,
    "start_time": "2021-10-14T07:10:05.970Z"
   },
   {
    "duration": 3887,
    "start_time": "2021-10-14T07:10:11.906Z"
   },
   {
    "duration": 18,
    "start_time": "2021-10-14T07:10:15.795Z"
   },
   {
    "duration": 21,
    "start_time": "2021-10-14T07:10:15.815Z"
   },
   {
    "duration": 360,
    "start_time": "2021-10-14T07:10:15.838Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-14T07:10:16.202Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-14T07:10:16.212Z"
   },
   {
    "duration": 409,
    "start_time": "2021-10-14T07:10:16.218Z"
   },
   {
    "duration": 8554,
    "start_time": "2021-10-14T07:13:29.762Z"
   },
   {
    "duration": 6176,
    "start_time": "2021-10-14T07:13:38.319Z"
   },
   {
    "duration": 3185,
    "start_time": "2021-10-14T07:13:44.498Z"
   },
   {
    "duration": 18,
    "start_time": "2021-10-14T07:13:47.686Z"
   },
   {
    "duration": 23,
    "start_time": "2021-10-14T07:13:47.707Z"
   },
   {
    "duration": 363,
    "start_time": "2021-10-14T07:13:47.733Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-14T07:13:48.098Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-14T07:13:48.108Z"
   },
   {
    "duration": 79,
    "start_time": "2021-10-14T07:13:48.116Z"
   },
   {
    "duration": 412747,
    "start_time": "2021-10-14T07:13:48.197Z"
   },
   {
    "duration": 7875,
    "start_time": "2021-10-14T07:20:40.946Z"
   },
   {
    "duration": 262,
    "start_time": "2021-10-14T07:20:48.824Z"
   },
   {
    "duration": 655,
    "start_time": "2021-10-14T07:20:48.434Z"
   },
   {
    "duration": 654,
    "start_time": "2021-10-14T07:20:48.436Z"
   },
   {
    "duration": 648,
    "start_time": "2021-10-14T07:20:48.444Z"
   },
   {
    "duration": 648,
    "start_time": "2021-10-14T07:20:48.445Z"
   },
   {
    "duration": 647,
    "start_time": "2021-10-14T07:20:48.447Z"
   },
   {
    "duration": 648,
    "start_time": "2021-10-14T07:20:48.448Z"
   },
   {
    "duration": 648,
    "start_time": "2021-10-14T07:20:48.449Z"
   },
   {
    "duration": 648,
    "start_time": "2021-10-14T07:20:48.450Z"
   },
   {
    "duration": 647,
    "start_time": "2021-10-14T07:20:48.453Z"
   },
   {
    "duration": 647,
    "start_time": "2021-10-14T07:20:48.454Z"
   },
   {
    "duration": 648,
    "start_time": "2021-10-14T07:20:48.455Z"
   },
   {
    "duration": 648,
    "start_time": "2021-10-14T07:20:48.456Z"
   },
   {
    "duration": 649,
    "start_time": "2021-10-14T07:20:48.457Z"
   },
   {
    "duration": 649,
    "start_time": "2021-10-14T07:20:48.458Z"
   },
   {
    "duration": 639,
    "start_time": "2021-10-14T07:26:15.539Z"
   },
   {
    "duration": 3,
    "start_time": "2021-10-14T07:28:17.041Z"
   },
   {
    "duration": 65,
    "start_time": "2021-10-14T07:28:18.431Z"
   },
   {
    "duration": 6828,
    "start_time": "2021-10-14T07:29:27.984Z"
   },
   {
    "duration": 6001,
    "start_time": "2021-10-14T07:29:34.815Z"
   },
   {
    "duration": 3209,
    "start_time": "2021-10-14T07:29:40.819Z"
   },
   {
    "duration": 32,
    "start_time": "2021-10-14T07:29:44.031Z"
   },
   {
    "duration": 25,
    "start_time": "2021-10-14T07:29:44.067Z"
   },
   {
    "duration": 345,
    "start_time": "2021-10-14T07:29:44.094Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-14T07:29:44.441Z"
   },
   {
    "duration": 13,
    "start_time": "2021-10-14T07:29:44.450Z"
   },
   {
    "duration": 70,
    "start_time": "2021-10-14T07:29:44.465Z"
   },
   {
    "duration": 441187,
    "start_time": "2021-10-14T07:29:44.537Z"
   },
   {
    "duration": 7661,
    "start_time": "2021-10-14T07:37:53.044Z"
   },
   {
    "duration": 6054,
    "start_time": "2021-10-14T07:38:00.707Z"
   },
   {
    "duration": 1838,
    "start_time": "2021-10-14T07:38:06.769Z"
   },
   {
    "duration": 19,
    "start_time": "2021-10-14T07:38:08.609Z"
   },
   {
    "duration": 36,
    "start_time": "2021-10-14T07:38:08.630Z"
   },
   {
    "duration": 545,
    "start_time": "2021-10-14T07:38:08.668Z"
   },
   {
    "duration": 8,
    "start_time": "2021-10-14T07:38:09.219Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-14T07:38:09.232Z"
   },
   {
    "duration": 427,
    "start_time": "2021-10-14T07:38:09.240Z"
   },
   {
    "duration": 491706,
    "start_time": "2021-10-14T07:38:09.670Z"
   },
   {
    "duration": 234,
    "start_time": "2021-10-14T07:53:47.864Z"
   },
   {
    "duration": 298,
    "start_time": "2021-10-14T07:54:32.930Z"
   },
   {
    "duration": 6989,
    "start_time": "2021-10-14T07:54:41.035Z"
   },
   {
    "duration": 5801,
    "start_time": "2021-10-14T07:54:49.177Z"
   },
   {
    "duration": 3776,
    "start_time": "2021-10-14T07:54:54.981Z"
   },
   {
    "duration": 20,
    "start_time": "2021-10-14T07:54:58.760Z"
   },
   {
    "duration": 19,
    "start_time": "2021-10-14T07:55:01.827Z"
   },
   {
    "duration": 319,
    "start_time": "2021-10-14T07:55:04.630Z"
   },
   {
    "duration": 14,
    "start_time": "2021-10-14T07:55:04.952Z"
   },
   {
    "duration": 4,
    "start_time": "2021-10-14T07:55:06.486Z"
   },
   {
    "duration": 400,
    "start_time": "2021-10-14T07:55:08.209Z"
   },
   {
    "duration": 402042,
    "start_time": "2021-10-14T07:55:09.333Z"
   },
   {
    "duration": 259,
    "start_time": "2021-10-14T08:18:04.091Z"
   },
   {
    "duration": 6537,
    "start_time": "2021-10-14T08:18:11.751Z"
   },
   {
    "duration": 6063,
    "start_time": "2021-10-14T08:18:18.291Z"
   },
   {
    "duration": 3551,
    "start_time": "2021-10-14T08:18:24.357Z"
   },
   {
    "duration": 19,
    "start_time": "2021-10-14T08:18:27.911Z"
   },
   {
    "duration": 19,
    "start_time": "2021-10-14T08:18:27.932Z"
   },
   {
    "duration": 348,
    "start_time": "2021-10-14T08:18:27.954Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-14T08:18:28.304Z"
   },
   {
    "duration": 6,
    "start_time": "2021-10-14T08:18:28.314Z"
   },
   {
    "duration": 433,
    "start_time": "2021-10-14T08:18:28.323Z"
   },
   {
    "duration": 406319,
    "start_time": "2021-10-14T08:18:28.759Z"
   },
   {
    "duration": 6670,
    "start_time": "2021-10-14T08:33:29.914Z"
   },
   {
    "duration": 6370,
    "start_time": "2021-10-14T08:33:36.587Z"
   },
   {
    "duration": 3887,
    "start_time": "2021-10-14T08:33:42.959Z"
   },
   {
    "duration": 23,
    "start_time": "2021-10-14T08:33:46.849Z"
   },
   {
    "duration": 18,
    "start_time": "2021-10-14T08:33:46.874Z"
   },
   {
    "duration": 371,
    "start_time": "2021-10-14T08:33:46.894Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-14T08:33:47.268Z"
   },
   {
    "duration": 20,
    "start_time": "2021-10-14T08:33:47.277Z"
   },
   {
    "duration": 417,
    "start_time": "2021-10-14T08:33:47.299Z"
   },
   {
    "duration": 430980,
    "start_time": "2021-10-14T08:33:47.719Z"
   },
   {
    "duration": 7626,
    "start_time": "2021-10-14T08:44:06.471Z"
   },
   {
    "duration": 6326,
    "start_time": "2021-10-14T08:44:14.101Z"
   },
   {
    "duration": 7106,
    "start_time": "2021-10-14T08:44:20.430Z"
   },
   {
    "duration": 23,
    "start_time": "2021-10-14T08:44:27.539Z"
   },
   {
    "duration": 19,
    "start_time": "2021-10-14T08:44:27.564Z"
   },
   {
    "duration": 394,
    "start_time": "2021-10-14T08:44:27.585Z"
   },
   {
    "duration": 7,
    "start_time": "2021-10-14T08:44:27.982Z"
   },
   {
    "duration": 5,
    "start_time": "2021-10-14T08:44:27.993Z"
   },
   {
    "duration": 439,
    "start_time": "2021-10-14T08:44:28.002Z"
   },
   {
    "duration": 413851,
    "start_time": "2021-10-14T08:44:28.444Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
